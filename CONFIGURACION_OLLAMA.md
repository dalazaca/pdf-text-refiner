# Configuraci√≥n de Ollama desde WSL

## Problema Detectado

Tu IP del host Windows es: **10.255.255.254**

Ollama est√° instalado en Windows pero no es accesible desde WSL porque est√° configurado para escuchar solo en `localhost` (127.0.0.1) de Windows, no en todas las interfaces de red.

## Soluci√≥n: Configurar Ollama para Escuchar en Todas las Interfaces

### Opci√≥n 1: Configurar Variable de Entorno en Windows (RECOMENDADO)

1. **Abrir PowerShell como Administrador** en Windows

2. **Configurar Ollama para escuchar en todas las interfaces:**
   ```powershell
   [System.Environment]::SetEnvironmentVariable('OLLAMA_HOST', '0.0.0.0:11434', 'User')
   ```

3. **Reiniciar el servicio de Ollama:**
   - Cierra Ollama completamente (busca el √≠cono en la bandeja del sistema y cierra)
   - Vuelve a abrir Ollama

4. **Permitir acceso en el Firewall:**
   ```powershell
   New-NetFirewallRule -DisplayName "Ollama WSL Access" -Direction Inbound -LocalPort 11434 -Protocol TCP -Action Allow
   ```

### Opci√≥n 2: Modo Mirrored Network (Windows 11 22H2+)

Si tienes Windows 11 versi√≥n 22H2 o superior, puedes habilitar el modo de red "mirrored":

1. Crea o edita el archivo `.wslconfig` en tu carpeta de usuario de Windows:
   ```
   C:\Users\TuUsuario\.wslconfig
   ```

2. Agrega estas l√≠neas:
   ```ini
   [wsl2]
   networkingMode=mirrored
   ```

3. Reinicia WSL desde PowerShell:
   ```powershell
   wsl --shutdown
   ```

4. Vuelve a abrir WSL. Ahora podr√°s acceder a Ollama usando `localhost:11434`

---

## Verificar la Conexi√≥n desde WSL

Una vez configurado, prueba la conexi√≥n desde WSL:

```bash
# Probar con la IP del host Windows
curl http://10.255.255.254:11434/api/tags

# Si configuraste modo mirrored, usa localhost
curl http://localhost:11434/api/tags
```

Deber√≠as ver una respuesta JSON con los modelos instalados.

---

## Modelos Recomendados para Correcci√≥n en Espa√±ol

Una vez que Ollama est√© accesible, descarga modelos optimizados para espa√±ol:

```bash
# En Windows (PowerShell):
ollama pull llama3.2:3b        # R√°pido y eficiente (3B par√°metros)
ollama pull mistral:7b         # Mejor calidad (7B par√°metros)
ollama pull gemma2:9b          # Excelente para espa√±ol (9B par√°metros)
```

**Recomendaci√≥n**: Empieza con `llama3.2:3b` para pruebas r√°pidas.

---

## Script de Prueba R√°pida

Guarda este script como `test_ollama.py` y ejec√∫talo desde WSL para verificar:

```python
#!/usr/bin/env python3
import requests

# Cambia seg√∫n tu configuraci√≥n:
OLLAMA_HOST = "http://10.255.255.254:11434"  # IP del host Windows
# O si usas modo mirrored:
# OLLAMA_HOST = "http://localhost:11434"

def test_connection():
    try:
        # Probar conexi√≥n
        response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=5)

        if response.status_code == 200:
            models = response.json().get('models', [])
            print("‚úÖ Conexi√≥n exitosa a Ollama!")
            print(f"üì¶ Modelos disponibles: {len(models)}")
            for model in models:
                print(f"   - {model['name']}")
            return True
        else:
            print(f"‚ùå Error: Status code {response.status_code}")
            return False

    except requests.exceptions.ConnectionError:
        print("‚ùå No se pudo conectar a Ollama")
        print(f"   Verifica que Ollama est√© corriendo en: {OLLAMA_HOST}")
        print("   Revisa la configuraci√≥n en este documento.")
        return False
    except Exception as e:
        print(f"‚ùå Error inesperado: {e}")
        return False

if __name__ == "__main__":
    test_connection()
```

Ejecuta:
```bash
python test_ollama.py
```

---

## Integraci√≥n con el C√≥digo Actual

Una vez verificada la conexi√≥n, instala la librer√≠a oficial de Ollama:

```bash
uv add ollama
```

El c√≥digo se modificar√° para:
1. Mantener LanguageTool para errores ortogr√°ficos b√°sicos
2. Agregar Ollama para an√°lisis profundo de redacci√≥n y coherencia
3. Combinar ambos resultados en el reporte final

---

## Troubleshooting

### "Connection refused" desde WSL
- Verifica que `OLLAMA_HOST` est√© configurado a `0.0.0.0`
- Confirma que el firewall permite el puerto 11434
- Reinicia Ollama despu√©s de cambiar configuraci√≥n

### "Model not found"
- Descarga el modelo primero: `ollama pull llama3.2:3b` en Windows

### Ollama muy lento
- Usa modelos m√°s peque√±os (3B en lugar de 7B)
- Verifica que tu GPU est√© siendo utilizada en Task Manager

### Error de timeout
- Aumenta el timeout en el c√≥digo Python
- Verifica que Ollama no est√© procesando otra solicitud
